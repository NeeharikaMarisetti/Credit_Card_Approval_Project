# -*- coding: utf-8 -*-
"""Capstone_project_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EDL1rUpBHv1GrjCjC_oJIsPAzyih12qg
"""

#Importing libraries needed for the project
import pandas as pd                           #For data manipulation and analysis
import numpy as np                            #For Mathematical computation
import seaborn as sns                         # For Visualization
import matplotlib.pyplot as plt               # For Visualization

#Reading dataset using pandas -data of independent variables
df1=pd.read_csv("/content/drive/MyDrive/project_01/Credit_card.csv")
#Reading dataset using pandas -data of output variable/labels
df2=pd.read_csv("/content/drive/MyDrive/project_01/Credit_card_label.csv")

#EDA
#KNOWING DATA
df1.head()

df = pd.merge(df1, df2, on='Ind_ID')
df.columns

df1.shape

print(df1['Ind_ID'].value_counts)
print(df1['Ind_ID'].isna().sum())

print(df1['GENDER'].value_counts())
print("-------")                                         ######Has NULL VALUES
print(df1['GENDER'].isna().sum())
print("-------")
sns.countplot(x=df['GENDER'])
plt.show()

contingency_table = pd.crosstab(df['GENDER'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

"""Insights: More female members applied for the credit card, and most of them were approved.

"""

print(df1['Car_Owner'].value_counts())
print("-------")
print(df1['Car_Owner'].isna().sum())
print("-------")
sns.countplot(x=df['Car_Owner'])
plt.show()

contingency_table = pd.crosstab(df['Car_Owner'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

print(df1['Propert_Owner'].value_counts())
print("-------")
print(df1['Propert_Owner'].isna().sum())
sns.countplot(x=df['Propert_Owner'])
plt.show()

contingency_table = pd.crosstab(df['Propert_Owner'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

print(df['CHILDREN'].value_counts())
print("-------")                                                             #Has an   OUTLIER
print(df['CHILDREN'].isna().sum())
print("__________")
sns.countplot(x=df['CHILDREN'])
plt.show()

"""Removing the row where children =14, as it is the only and highly rare to have 14 children, and it is row having extreme value removing it from the data


"""

df=df[df['CHILDREN']<=4]
print(df['CHILDREN'].value_counts())
print("-------")
print(df['CHILDREN'].isna().sum())
print("__________")
sns.countplot(x=df['CHILDREN'])
plt.show()

contingency_table = pd.crosstab(df['CHILDREN'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

"""Insights: For most of the  people who applied for credit card have no children."""

print(df1['Annual_income'].value_counts())
print("-------")                                                               #####Has MISSING VALUES
print(df1['Annual_income'].isna().sum())
print("_________")
sns.histplot(df['Annual_income'])
plt.show()

import plotly.express as px
fig= px.box(y=df['Annual_income'])
fig.show()

df[df['Annual_income']>225000]['Type_Income'].value_counts()          # finding which type income has annual income above Q3

df[df['Annual_income']>225000]['Housing_type'].value_counts()       # finding which housing type has annual income above Q3

df[df['Annual_income']>225000]['Type_Occupation'].value_counts()    # finding which occupation has annual income above Q3

"""Observation: There is no particular occupation which has above Q3 annual income. Various occupations have high incomes from labrores to managers to high skill tech staff

"""

px.box(x=df['label'],y=df['Annual_income'])

px.box(x=df['Type_Occupation'],y=df['Annual_income'])

px.box(x=df['Family_Members'],y=df['Annual_income'])

print(df['Type_Income'].value_counts())
print("-------")
print(df['Type_Income'].isna().sum())

contingency_table = pd.crosstab(df['Type_Income'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

"""Observation: Out of all income types, working group has most number of credit cards.

"""

#EDUCATION	Marital_status	Housing_type	Birthday_count
print(df['EDUCATION'].value_counts())
print("-------")
print(df['EDUCATION'].isna().sum())
print("_____")
plt.figure(figsize=(12, 6))
sns.countplot(x=df['EDUCATION'])
plt.show()

contingency_table = pd.crosstab(df['EDUCATION'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

plt.figure(figsize=(15,6))
sns.countplot(x=df['EDUCATION'],hue=df['Type_Occupation'])
plt.show()

"""Observation: Most people who applied and got credit card approval are from secondary education."""

print(df1['Marital_status'].value_counts())
print("-------")
print(df1['Marital_status'].isna().sum())
print("-----------")
plt.figure(figsize=(12,6))
sns.countplot(x=df['Marital_status'])
plt.show()

contingency_table = pd.crosstab(df['Marital_status'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

"""Observation: Most people who applied and got credit card approval in Marital_status category are from Married category."""

print(df1['Housing_type'].value_counts())
print("-------")
print(df1['Housing_type'].isna().sum())
print("-----------")
plt.figure(figsize=(12,6))
sns.countplot(x=df['Housing_type'])
plt.show()

"""Observation: People with house type House/apartment are majority who applied and recieved credit card approval."""

contingency_table = pd.crosstab(df['Housing_type'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

print(df1['Birthday_count'].value_counts())
print("-------")
print(df1['Birthday_count'].isna().sum())
print("-------")
result_boolean = df1['Birthday_count'] < 0
print(df1[result_boolean]['Birthday_count'].shape)



df['Birthday_count'].isnull().sum()

#Employed_days	Mobile_phone	Work_Phone	Phone	EMAIL_ID
print(df['Employed_days'].value_counts())
print("-------")
print(df['Employed_days'].isna().sum())
print("-------")

print(df[df['Employed_days']>0].shape)



sns.scatterplot(x=df['Birthday_count'],y=df['Employed_days'])
plt.show()

print(df1['Type_Occupation'].value_counts())
print("-------")
print(df1['Type_Occupation'].isna().sum())
print("-------")
plt.figure(figsize=(20,6))
sns.countplot(x=df['Type_Occupation'])
plt.show()

contingency_table = pd.crosstab(df['Type_Occupation'], df['label'])
print(contingency_table)
sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
plt.show()

print(df1['Family_Members'].value_counts())
print("-------")
print(df1['Family_Members'].isna().sum())
print("-------")
plt.figure(figsize=(10,6))
sns.countplot(x=df['Family_Members'])
plt.show()

df[df['Family_Members']>6]            #What to do with this data?

"""Though this is valid extreme value, It is an outlier for family members and children categorical column, so for easier compuational efficiency removing the column"""

df = df[df['Family_Members'] <=6]

print(df['Mobile_phone'].value_counts())
print("-------")
print(df['Mobile_phone'].isna().sum())
print("-------")
sns.countplot(x=df['Mobile_phone'])
plt.show()

print(df1['Work_Phone'].value_counts())
print("-------")
print(df1['Work_Phone'].isna().sum())
print("-------")

print(df1['Phone'].value_counts())
print("-------")
print(df1['Phone'].isna().sum())
print("-------")

#EMAIL_ID	Type_Occupation	Family_Members
print(df1['EMAIL_ID'].value_counts())
print("-------")
print(df1['EMAIL_ID'].isna().sum())
print("-------")

print(df1['Type_Occupation']['Annual_income'].value_counts())
print("-------")                                                              ###This type occupation group by annual income and perform missing value data
print(df1['Type_Occupation'].isna().sum())
print("-------")

print(df1['Family_Members'].value_counts())
print("-------")
print(df1['Family_Members'].isna().sum())

df.info()

df.isnull().sum()



"""##Droping Unnecessary columns"""

#columns to drop:
1. Ind_ID is just an ID number which is not useful for prediction so dropping it
2. Mobile_Phone, work-phone, phone and email-id are for contacting purpose only. so, dropping them

df.head()



result_boolean =  df['label'] ==1
df[result_boolean]['Mobile_phone'].value_counts()

df[result_boolean]['label'].value_counts()

df['Mobile_phone'].value_counts()

res = df['label']==1
df[res]['EMAIL_ID'].value_counts()

"""  Checking the datatypes of the column and values present inside columns using info() and head() functions"""

df.info()

df.head(5)

df['Type_Occupation'].value_counts()

"""Observation: Data type matched

"""

df[df.duplicated()] ## Checking for duplicate values

"""Observation: No duplicate values are present"""

#Knowing the columns and their distribution and relation between independent variables using graphs
#Numerical(Univariate)- boxplot, histogram and distplot
#Categorical(Univariate)- barplot, countplot, pieplot
#Numericateal-Cgorical - Bar,box,violin
#Numerical-Numerical- Scatter plot, Line plot
#Categorical-Categorical- Heat map in conjuction with cross tab

df.columns



df.isnull().sum()

"""#removing unwanted columns"""

from scipy.stats import chi2_contingency
contingency_table = pd.crosstab(df['Mobile_phone'], df['label'])
chi2, p, _, _ = chi2_contingency(contingency_table)
print(f"Chi-square value: {chi2}")
print(f"P-value: {p}")

"""Since, alpha=0.05 and p-value is 1, p-value>alpha, which implies our variables are independent of each other. So, dropping the variable 'Mobile_phone'"""

from scipy.stats import chi2_contingency
chi2, p, _, _ = chi2_contingency(contingency_table)
print(f"Chi-square value: {chi2}")
print(f"P-value: {p}")

contingency_table = pd.crosstab(df['Work_Phone'], df['label'])
print(contingency_table)
#sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
#plt.show()
from scipy.stats import chi2_contingency
chi2, p, _, _ = chi2_contingency(contingency_table)
print(f"Chi-square value: {chi2}")
print(f"P-value: {p}")

"""Since, alpha=0.05 and p-value is 0.85, p-value>alpha, which implies our variables are independent of each other. So, dropping the variable 'Work_phone'"""



contingency_table = pd.crosstab(df['Phone'], df['label'])
print(contingency_table)
#sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
#plt.show()
from scipy.stats import chi2_contingency
chi2, p, _, _ = chi2_contingency(contingency_table)
print(f"Chi-square value: {chi2}")
print(f"P-value: {p}")

"""Since, alpha=0.05 and p-value is 0.85, p-value>alpha, which implies our variables are independent of each other. So, dropping the variable 'Phone'"""

contingency_table = pd.crosstab(df['label'], df['EMAIL_ID'])
print(contingency_table)
#sns.heatmap(contingency_table, annot=True, cmap='YlGnBu', fmt='d')
#plt.show()
from scipy.stats import chi2_contingency
chi2, p, _, _ = chi2_contingency(contingency_table)
print(f"Chi-square value: {chi2}")
print(f"P-value: {p}")

"""Since, alpha=0.05 and p-value is 0.711, p-value>alpha, which implies our variables are independent of each other. So, dropping the variable 'Email_ID"""

#Dropping the columns Mobile_phone, Phone,Work_Phone, EMAIL_ID
df.drop(columns=["Mobile_phone","Phone","Work_Phone","EMAIL_ID"], inplace=True)

#Dropping the column Ind_ID too as it is only an ID and it does not contribute or effect to credit card approval
df.drop(columns=['Ind_ID'], inplace=True)

df.columns

df.isnull().sum()

df.describe()

df['GENDER']=df['GENDER'].fillna(df['GENDER'].mode()[0])

df.isnull().sum()

df['Type_Occupation']=df['Type_Occupation'].fillna(df['Type_Occupation'].mode()[0])

df['Type_Occupation'].value_counts()

df.isnull().sum()

sns.boxplot(df['Annual_income'])
plt.show()

df['Annual_income']=df['Annual_income'].fillna(df['Annual_income'].mean())

df.isnull().sum()

sns.boxplot(df['Birthday_count'])
plt.show()

df['Birthday_count']=round(df['Birthday_count']/(-365))        #Converting birthday count from days to years as age.

df['Birthday_count']=df['Birthday_count'].fillna(df['Birthday_count'].mean())

plt.hist(df['Birthday_count'])

import plotly.express as px
fig= px.box(y=df['Birthday_count'])
fig.show()

df.mean= df.copy()
df.mean['Birthday_count']=df.mean['Birthday_count'].fillna(round(df.mean['Birthday_count'].mean()))
df.median=df.copy()
df.median['Birthday_count']=df.median['Birthday_count'].fillna(round(df.median['Birthday_count'].median()))

plt.figure(figsize=(12,6))
plt.subplot(1,3,1)
plt.hist(df['Birthday_count'])
plt.subplot(1,3,2)
plt.hist(df.mean['Birthday_count'])
plt.subplot(1,3,3)
plt.hist(df.median['Birthday_count'])
plt.show()

"""Observation: filling the missing value with mean or median results in almost same distribution of birday count, so choosing mean"""

df.isnull().sum()

df.info()

df['Birthday_count']=df['Birthday_count'].astype("int")



df['Employed_days'] = df['Employed_days'].apply(lambda x: 0 if x >0 else round(x/(-365)))

df['Employed_days'].value_counts()

plt.hist(df['Employed_days'])

plt.scatter(y=df['Employed_days'], x=df['Birthday_count'])



"""Observation: Data is correctly alligned, as birthdaty count is directly proportional to Employed days."""

df= df[df['CHILDREN']<=4]     # there ia an outlier for children where child count is 14, though data is correct and valid, removing the row for better performance of the model.

"""Handling outliers in annual income and employed day column"""

df.describe()

# for employed days handling outliers using quartile range
q3=df.describe()['Employed_days']['75%']
q1=df.describe()['Employed_days']['25%']
IQR=q3-q1
lower_limit=q1-(1.5*IQR)
Upper_limit=q3+(1.5*IQR)
df['Employed_days']=df['Employed_days'].clip(lower_limit,Upper_limit)

import plotly.express as px
fig= px.box(y=df['Employed_days'])
fig.show()

# for Annual Income handling outliers using quartile range
q3=df.describe()['Annual_income']['75%']
q1=df.describe()['Annual_income']['25%']
IQR=q3-q1
lower_limit=q1-(1.5*IQR)
Upper_limit=q3+(1.5*IQR)
df['Annual_income']=df['Annual_income'].clip(lower_limit,Upper_limit)

fig= px.box(y=df['Annual_income'])
fig.show()

"""#Encoding, the process of converting text data to numerical data for the model to comprehend."""

df['label'].value_counts()

df.columns

Nominal Data:              #pd.get_dummies or one hot encoding
- GENDER
- Car_Owner
- Propert_Owner
- Type_Income
- EDUCATION
- Marital_status
- Housing_type
- Type_Occupation
Ordinal Data:             #apply map or ordinal encoder, since ordinal columns are in numericals, not changing them using them as it is.
- CHILDREN
- Family_Members
Label-                    #label encoding



df=pd.get_dummies(df, columns=["GENDER", "Car_Owner", "Propert_Owner","Type_Income","EDUCATION","Type_Occupation","Housing_type","Marital_status"], drop_first=True)

df.head()

#To bring all the values to same range we use standardization(mean=0 and std=1) or normalization.
#Feature scaling techniques.

X= df.drop(columns='label', axis=1)
Y=df['label']



Y.value_counts()

#Split the data into train and test using hold out method
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2, random_state=0)

#x_train=80%, x_test=20%
#y_train=80%, y_test=20%

from sklearn.linear_model import LogisticRegression
model_01 = LogisticRegression()
model_01.fit(x_train,y_train)

y_pred_01= model_01.predict(x_test)

from sklearn.metrics import accuracy_score

print(accuracy_score(y_test,y_pred_01))

from sklearn.tree import DecisionTreeClassifier
model2 = DecisionTreeClassifier()
model2.fit(x_train,y_train)

y_pred_02= model2.predict(x_test)

print(accuracy_score(y_test,y_pred_02))

from sklearn.ensemble import RandomForestClassifier
model3 = RandomForestClassifier()
model3.fit(x_train,y_train)

y_pred_03= model3.predict(x_test)

print(accuracy_score(y_test,y_pred_03))

from xgboost import XGBClassifier
model4 = XGBClassifier()
model4.fit(x_train,y_train)

y_pred_04= model4.predict(x_test)

print(accuracy_score(y_test,y_pred_04))

df.columns

df1.columns

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the data
scaled_x_train = scaler.fit_transform(x_train)
scaled_x_test = scaler.transform(x_test)
x_train_scaled=pd.DataFrame(scaled_x_train, columns=x_test.columns)
x_test_scaled=pd.DataFrame(scaled_x_test, columns=x_train.columns)

from sklearn.linear_model import LogisticRegression
model_01 = LogisticRegression()
model_01.fit(x_train_scaled,y_train)

y_pred_01= model_01.predict(x_test_scaled)

from sklearn.metrics import precision_score
precision_LR = precision_score(y_true, y_pred)











import duckdb
conn=duckdb.connect()

conn.register("df",df)

conn.execute("select * from df").fetchdf()